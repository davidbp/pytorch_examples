{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Build-a-DataLoader\" data-toc-modified-id=\"Build-a-DataLoader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Build a <code>DataLoader</code></a></span></li><li><span><a href=\"#Building-a-Convnet\" data-toc-modified-id=\"Building-a-Convnet-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Building a Convnet</a></span></li><li><span><a href=\"#Training-model\" data-toc-modified-id=\"Training-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:01.241022Z",
     "start_time": "2020-12-09T19:27:59.893769Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.142184Z",
     "start_time": "2020-12-09T19:28:01.242906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/train.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.536379Z",
     "start_time": "2020-12-09T19:28:04.146306Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "X = df.drop(['label'],1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.541146Z",
     "start_time": "2020-12-09T19:28:04.538717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6300,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.548229Z",
     "start_time": "2020-12-09T19:28:04.542823Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "torch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\n",
    "torch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "torch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\n",
    "torch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(torch_X_train, torch_y_train)\n",
    "test = torch.utils.data.TensorDataset(torch_X_test, torch_y_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.559518Z",
     "start_time": "2020-12-09T19:28:04.549685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.564665Z",
     "start_time": "2020-12-09T19:28:04.561021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Convnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know the input dimensions of the dense layers we need to find the ouptut shape of the convolution and maxpooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:32:24.120337Z",
     "start_time": "2020-12-09T19:32:24.111789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n",
      "torch.Size([10, 20, 24, 24])\n",
      "torch.Size([10, 20, 12, 12])\n",
      "torch.Size([10, 50, 8, 8])\n",
      "torch.Size([10, 50, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, \n",
    "                  out_channels=20,\n",
    "                  kernel_size=5,\n",
    "                  stride=1)\n",
    "        \n",
    "conv2 = nn.Conv2d(in_channels=20, \n",
    "                  out_channels=50,\n",
    "                  kernel_size=5,\n",
    "                  stride=1)\n",
    "\n",
    "n_batch = 10\n",
    "x = torch.tensor(X_train[0:n_batch]).float()\n",
    "x = x.reshape(n_batch,1,28,28)\n",
    "\n",
    "print(x.shape)\n",
    "x = F.relu(conv1(x))\n",
    "print(x.shape)\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print(x.shape)\n",
    "x = F.relu(conv2(x))\n",
    "print(x.shape)\n",
    "x = F.max_pool2d(x, 2, 2)\n",
    "print(x.shape) # (((28-4)/2)-4)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:18:11.421799Z",
     "start_time": "2020-12-09T19:18:11.418665Z"
    }
   },
   "source": [
    "- Since there is no padding each time a kernel is applied the output volume will be shrinked substracting kernel_size -1 to its height and width.\n",
    "\n",
    "- Each maxpool layer will divide by 2 the volume size\n",
    "\n",
    "\n",
    "In summary\n",
    "\n",
    "- input size        `n_batchx1x28x28`\n",
    "- Output conv1:     `n_batch x n_filters_conv1 x 24 x 24`, the 24 comes from 28-(5-1)\n",
    "- Output maxpool1:  `n_batch x n_filters_conv1 x 12 x 12`, the 12 comes from 24/2   \n",
    "- Output conv2:     `n_batch x n_filters_conv2 x 8 x 8`, the 8 comes from 12 - (5-1)\n",
    "- Output maxpool2:  `n_batch x n_filters_conv2 x 4 x 4`, the 4 comes from 8/2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.589496Z",
     "start_time": "2020-12-09T19:28:04.581552Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvNet -> Max_Pool -> RELU -> ConvNet -> Max_Pool -> RELU \n",
    "            -> FC -> RELU -> FC -> SOFTMAX\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=20,\n",
    "                               kernel_size=5,\n",
    "                               stride=1)\n",
    "        \n",
    "        # H x W x C conv1  28 x 28 x 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, \n",
    "                               out_channels=50,\n",
    "                               kernel_size=5,\n",
    "                               stride=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "# RuntimeError: Given groups=1, \n",
    "# weight of size [6, 3, 5, 5],  expected input[10, 1, 28, 28]\n",
    "# to have 3 channels, but got 1 channels instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.597742Z",
     "start_time": "2020-12-09T19:28:04.591322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.609643Z",
     "start_time": "2020-12-09T19:28:04.604276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "loss_func   = nn.CrossEntropyLoss()\n",
    "optimizer   = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "var_X_batch = torch.Tensor(X_train[0:10]).float()\n",
    "var_X_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model takes as input a tensor of size [n_batch, 1, 28,28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.616682Z",
     "start_time": "2020-12-09T19:28:04.611928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_X_batch = torch.Tensor(X_train[0:10]).float()\n",
    "var_X_batch = var_X_batch.reshape(10,1,28,28)\n",
    "var_X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:04.627458Z",
     "start_time": "2020-12-09T19:28:04.618989Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, train_loader, optimizer, loss_func, n_epochs):\n",
    "    model.train()\n",
    "    n_batch = train_loader.batch_size\n",
    "    for epoch in range(n_epochs):\n",
    "        correct = 0\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            n_batch = len(y_batch)\n",
    "            var_X_batch = Variable(X_batch).float()\n",
    "            var_X_batch = var_X_batch.reshape(n_batch,1,28,28)\n",
    "            var_y_batch = Variable(y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(var_X_batch)\n",
    "            loss_minibatch = loss_func(output, var_y_batch)\n",
    "            loss_minibatch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Total correct predictions\n",
    "            predicted = torch.max(output.data, 1)[1] \n",
    "            correct += (predicted == var_y_batch).sum()\n",
    "            #print(correct)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, batch_idx*len(X_batch), len(train_loader.dataset),\n",
    "                    100.*batch_idx / len(train_loader),\n",
    "                    loss_minibatch.data.item(),\n",
    "                    float(correct*100) / float(n_batch*(batch_idx+1))))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:38.488869Z",
     "start_time": "2020-12-09T19:28:04.629109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/35700 (0%)]\tLoss: 6.908583\t Accuracy:12.500%\n",
      "Epoch : 0 [1600/35700 (4%)]\tLoss: 2.367464\t Accuracy:18.137%\n",
      "Epoch : 0 [3200/35700 (9%)]\tLoss: 1.703300\t Accuracy:21.380%\n",
      "Epoch : 0 [4800/35700 (13%)]\tLoss: 1.037428\t Accuracy:33.609%\n",
      "Epoch : 0 [6400/35700 (18%)]\tLoss: 0.682627\t Accuracy:45.833%\n",
      "Epoch : 0 [8000/35700 (22%)]\tLoss: 0.386490\t Accuracy:53.897%\n",
      "Epoch : 0 [9600/35700 (27%)]\tLoss: 0.424029\t Accuracy:59.832%\n",
      "Epoch : 0 [11200/35700 (31%)]\tLoss: 0.224900\t Accuracy:64.521%\n",
      "Epoch : 0 [12800/35700 (36%)]\tLoss: 0.461954\t Accuracy:68.041%\n",
      "Epoch : 0 [14400/35700 (40%)]\tLoss: 0.339240\t Accuracy:70.926%\n",
      "Epoch : 0 [16000/35700 (45%)]\tLoss: 0.082056\t Accuracy:73.422%\n",
      "Epoch : 0 [17600/35700 (49%)]\tLoss: 0.136290\t Accuracy:75.493%\n",
      "Epoch : 0 [19200/35700 (54%)]\tLoss: 0.258625\t Accuracy:77.043%\n",
      "Epoch : 0 [20800/35700 (58%)]\tLoss: 0.136400\t Accuracy:78.399%\n",
      "Epoch : 0 [22400/35700 (63%)]\tLoss: 0.129545\t Accuracy:79.623%\n",
      "Epoch : 0 [24000/35700 (67%)]\tLoss: 0.296577\t Accuracy:80.684%\n",
      "Epoch : 0 [25600/35700 (72%)]\tLoss: 0.230883\t Accuracy:81.648%\n",
      "Epoch : 0 [27200/35700 (76%)]\tLoss: 0.026750\t Accuracy:82.513%\n",
      "Epoch : 0 [28800/35700 (81%)]\tLoss: 0.006016\t Accuracy:83.307%\n",
      "Epoch : 0 [30400/35700 (85%)]\tLoss: 0.216859\t Accuracy:84.020%\n",
      "Epoch : 0 [32000/35700 (90%)]\tLoss: 0.020067\t Accuracy:84.681%\n",
      "Epoch : 0 [33600/35700 (94%)]\tLoss: 0.071354\t Accuracy:85.231%\n",
      "Epoch : 0 [35200/35700 (99%)]\tLoss: 0.110323\t Accuracy:85.749%\n",
      "Epoch : 1 [0/35700 (0%)]\tLoss: 0.258065\t Accuracy:84.375%\n",
      "Epoch : 1 [1600/35700 (4%)]\tLoss: 0.137520\t Accuracy:95.650%\n",
      "Epoch : 1 [3200/35700 (9%)]\tLoss: 0.010239\t Accuracy:96.380%\n",
      "Epoch : 1 [4800/35700 (13%)]\tLoss: 0.015510\t Accuracy:96.544%\n",
      "Epoch : 1 [6400/35700 (18%)]\tLoss: 0.107873\t Accuracy:96.580%\n",
      "Epoch : 1 [8000/35700 (22%)]\tLoss: 0.145807\t Accuracy:96.663%\n",
      "Epoch : 1 [9600/35700 (27%)]\tLoss: 0.045544\t Accuracy:96.782%\n",
      "Epoch : 1 [11200/35700 (31%)]\tLoss: 0.011614\t Accuracy:96.991%\n",
      "Epoch : 1 [12800/35700 (36%)]\tLoss: 0.173236\t Accuracy:96.992%\n",
      "Epoch : 1 [14400/35700 (40%)]\tLoss: 0.108909\t Accuracy:97.111%\n",
      "Epoch : 1 [16000/35700 (45%)]\tLoss: 0.073880\t Accuracy:97.174%\n",
      "Epoch : 1 [17600/35700 (49%)]\tLoss: 0.007641\t Accuracy:97.266%\n",
      "Epoch : 1 [19200/35700 (54%)]\tLoss: 0.036846\t Accuracy:97.312%\n",
      "Epoch : 1 [20800/35700 (58%)]\tLoss: 0.095234\t Accuracy:97.278%\n",
      "Epoch : 1 [22400/35700 (63%)]\tLoss: 0.096533\t Accuracy:97.258%\n",
      "Epoch : 1 [24000/35700 (67%)]\tLoss: 0.183601\t Accuracy:97.250%\n",
      "Epoch : 1 [25600/35700 (72%)]\tLoss: 0.119964\t Accuracy:97.250%\n",
      "Epoch : 1 [27200/35700 (76%)]\tLoss: 0.011985\t Accuracy:97.283%\n",
      "Epoch : 1 [28800/35700 (81%)]\tLoss: 0.000913\t Accuracy:97.340%\n",
      "Epoch : 1 [30400/35700 (85%)]\tLoss: 0.151578\t Accuracy:97.374%\n",
      "Epoch : 1 [32000/35700 (90%)]\tLoss: 0.003292\t Accuracy:97.415%\n",
      "Epoch : 1 [33600/35700 (94%)]\tLoss: 0.065921\t Accuracy:97.440%\n",
      "Epoch : 1 [35200/35700 (99%)]\tLoss: 0.124596\t Accuracy:97.457%\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "fit(model, train_loader, optimizer, loss_func, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:38.493912Z",
     "start_time": "2020-12-09T19:28:38.490390Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    #model = mlp\n",
    "    correct = 0 \n",
    "    for test_imgs, test_labels in test_loader:\n",
    "        test_imgs = Variable(test_imgs).float()\n",
    "        test_imgs = test_imgs.reshape(len(test_labels),1,28,28)\n",
    "        output = model(test_imgs)\n",
    "        predicted = torch.max(output,1)[1]\n",
    "        correct += (predicted == test_labels).sum()\n",
    "    print(\"Test accuracy:{:.3f}% \".format( float(correct) / (len(test_loader)*BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:39.386628Z",
     "start_time": "2020-12-09T19:28:38.499383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.975% \n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:27:10.258329Z",
     "start_time": "2020-12-09T19:27:10.255567Z"
    }
   },
   "source": [
    "Another architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T19:28:39.397884Z",
     "start_time": "2020-12-09T19:28:39.391730Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        #self.conv1 = nn.Conv2d(1, 4, kernel_size = 3, stride=1, padding=1)\n",
    "        self.pool  = nn.MaxPool2d(2, 2)\n",
    "        #self.conv2 = nn.Conv2d(4, 4, kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170.1999969482422px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
