{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM tutorial pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) +  np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(0.7*x / 1.0 / T).astype('float64') \n",
    "torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wN/QpZGx8FsvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa68a2001d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and LSTMCell from pytorch\n",
    "\n",
    "About LSTM and LSTM cell\n",
    "\n",
    "- https://discuss.pytorch.org/t/different-between-lstm-and-lstmcell-function/5657\n",
    "\n",
    "\n",
    "About timeseries prediction\n",
    "\n",
    "- http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction\n",
    "\n",
    "\n",
    "\n",
    "    class LSTM(RNNBase)\n",
    "       Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
    "       sequence.\n",
    "       \n",
    "       \n",
    "       For each element in the input sequence, each layer computes the following\n",
    "       function:\n",
    "       \n",
    "       .. math::\n",
    "       \n",
    "               \\begin{array}{ll}\n",
    "               i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "               f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "               g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n",
    "               o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "               c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "               h_t = o_t * \\tanh(c_t)\n",
    "               \\end{array}\n",
    "       \n",
    "       where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
    "       state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n",
    "       time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n",
    "       :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n",
    "       and out gates, respectively.\n",
    "       \n",
    "       Args:\n",
    "           input_size: The number of expected features in the input x\n",
    "           hidden_size: The number of features in the hidden state h\n",
    "           num_layers: Number of recurrent layers.\n",
    "           bias: If False, then the layer does not use bias weights b_ih and b_hh.\n",
    "               Default: True\n",
    "           batch_first: If True, then the input and output tensors are provided\n",
    "               as (batch, seq, feature)\n",
    "           dropout: If non-zero, introduces a dropout layer on the outputs of each\n",
    "               RNN layer except the last layer\n",
    "           bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
    "       \n",
    "       Inputs: input, (h_0, c_0)\n",
    "           - **input** (seq_len, batch, input_size): tensor containing the features\n",
    "             of the input sequence.\n",
    "             The input can also be a packed variable length sequence.\n",
    "             See :func:`torch.nn.utils.rnn.pack_padded_sequence` for details.\n",
    "           - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
    "             containing the initial hidden state for each element in the batch.\n",
    "           - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
    "             containing the initial cell state for each element in the batch.\n",
    "       \n",
    "       \n",
    "       Outputs: output, (h_n, c_n)\n",
    "           - **output** (seq_len, batch, hidden_size * num_directions): tensor\n",
    "             containing the output features `(h_t)` from the last layer of the RNN,\n",
    "             for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
    "             given as the input, the output will also be a packed sequence.\n",
    "           - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "             containing the hidden state for t=seq_len\n",
    "           - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "             containing the cell state for t=seq_len\n",
    "       \n",
    "       Attributes:\n",
    "           weight_ih_l[k] : the learnable input-hidden weights of the k-th layer\n",
    "               `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)`\n",
    "           weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer\n",
    "               `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)`\n",
    "           bias_ih_l[k] : the learnable input-hidden bias of the k-th layer\n",
    "               `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
    "           bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer\n",
    "               `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
    "       \n",
    "       Examples::\n",
    "       \n",
    "           >>> rnn = nn.LSTM(10, 20, 2)\n",
    "           >>> input = Variable(torch.randn(5, 3, 10))\n",
    "           >>> h0 = Variable(torch.randn(2, 3, 20))\n",
    "           >>> c0 = Variable(torch.randn(2, 3, 20))\n",
    "           >>> output, hn = rnn(input, (h0, c0))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTM in module torch.nn.modules.rnn:\n",
      "\n",
      "class LSTM(RNNBase)\n",
      " |  Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
      " |  sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |          \\begin{array}{ll}\n",
      " |          i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
      " |          f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
      " |          g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n",
      " |          o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
      " |          c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
      " |          h_t = o_t * \\tanh(c_t)\n",
      " |          \\end{array}\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
      " |  state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n",
      " |  time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n",
      " |  :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n",
      " |  and out gates, respectively.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input x\n",
      " |      hidden_size: The number of features in the hidden state h\n",
      " |      num_layers: Number of recurrent layers.\n",
      " |      bias: If False, then the layer does not use bias weights b_ih and b_hh.\n",
      " |          Default: True\n",
      " |      batch_first: If True, then the input and output tensors are provided\n",
      " |          as (batch, seq, feature)\n",
      " |      dropout: If non-zero, introduces a dropout layer on the outputs of each\n",
      " |          RNN layer except the last layer\n",
      " |      bidirectional: If True, becomes a bidirectional RNN. Default: False\n",
      " |  \n",
      " |  Inputs: input, (h_0, c_0)\n",
      " |      - **input** (seq_len, batch, input_size): tensor containing the features\n",
      " |        of the input sequence.\n",
      " |        The input can also be a packed variable length sequence.\n",
      " |        See :func:`torch.nn.utils.rnn.pack_padded_sequence` for details.\n",
      " |      - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
      " |        containing the initial hidden state for each element in the batch.\n",
      " |      - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
      " |        containing the initial cell state for each element in the batch.\n",
      " |  \n",
      " |  \n",
      " |  Outputs: output, (h_n, c_n)\n",
      " |      - **output** (seq_len, batch, hidden_size * num_directions): tensor\n",
      " |        containing the output features `(h_t)` from the last layer of the RNN,\n",
      " |        for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
      " |        given as the input, the output will also be a packed sequence.\n",
      " |      - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
      " |        containing the hidden state for t=seq_len\n",
      " |      - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
      " |        containing the cell state for t=seq_len\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k] : the learnable input-hidden weights of the k-th layer\n",
      " |          `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)`\n",
      " |      weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer\n",
      " |          `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)`\n",
      " |      bias_ih_l[k] : the learnable input-hidden bias of the k-th layer\n",
      " |          `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
      " |      bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer\n",
      " |          `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.LSTM(10, 20, 2)\n",
      " |      >>> input = Variable(torch.randn(5, 3, 10))\n",
      " |      >>> h0 = Variable(torch.randn(2, 3, 20))\n",
      " |      >>> c0 = Variable(torch.randn(2, 3, 20))\n",
      " |      >>> output, hn = rnn(input, (h0, c0))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  flatten_parameters(self)\n",
      " |      Resets parameter data pointer so that they can use faster code paths.\n",
      " |      \n",
      " |      Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
      " |      Otherwise, it's a no-op.\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |  \n",
      " |  cuda(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device_id (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. The keys of :attr:`state_dict` must\n",
      " |      exactly match the keys returned by this module's :func:`state_dict()`\n",
      " |      function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time :func:`forward` computes an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='')\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTMCell in module torch.nn.modules.rnn:\n",
      "\n",
      "class LSTMCell(RNNCellBase)\n",
      " |  A long short-term memory (LSTM) cell.\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      \\begin{array}{ll}\n",
      " |      i = \\mathrm{sigmoid}(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\\n",
      " |      f = \\mathrm{sigmoid}(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\\n",
      " |      g = \\tanh(W_{ig} x + b_{ig} + W_{hc} h + b_{hg}) \\\\\n",
      " |      o = \\mathrm{sigmoid}(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\\n",
      " |      c' = f * c + i * g \\\\\n",
      " |      h' = o * \\tanh(c') \\\\\n",
      " |      \\end{array}\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input x\n",
      " |      hidden_size: The number of features in the hidden state h\n",
      " |      bias: If `False`, then the layer does not use bias weights `b_ih` and\n",
      " |          `b_hh`. Default: True\n",
      " |  \n",
      " |  Inputs: input, (h_0, c_0)\n",
      " |      - **input** (batch, input_size): tensor containing input features\n",
      " |      - **h_0** (batch, hidden_size): tensor containing the initial hidden\n",
      " |        state for each element in the batch.\n",
      " |      - **c_0** (batch. hidden_size): tensor containing the initial cell state\n",
      " |        for each element in the batch.\n",
      " |  \n",
      " |  Outputs: h_1, c_1\n",
      " |      - **h_1** (batch, hidden_size): tensor containing the next hidden state\n",
      " |        for each element in the batch\n",
      " |      - **c_1** (batch, hidden_size): tensor containing the next cell state\n",
      " |        for each element in the batch\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih: the learnable input-hidden weights, of shape\n",
      " |          `(4*hidden_size x input_size)`\n",
      " |      weight_hh: the learnable hidden-hidden weights, of shape\n",
      " |          `(4*hidden_size x hidden_size)`\n",
      " |      bias_ih: the learnable input-hidden bias, of shape `(4*hidden_size)`\n",
      " |      bias_hh: the learnable hidden-hidden bias, of shape `(4*hidden_size)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.LSTMCell(10, 20)\n",
      " |      >>> input = Variable(torch.randn(6, 3, 10))\n",
      " |      >>> hx = Variable(torch.randn(3, 20))\n",
      " |      >>> cx = Variable(torch.randn(3, 20))\n",
      " |      >>> output = []\n",
      " |      >>> for i in range(6):\n",
      " |      ...     hx, cx = rnn(input[i], (hx, cx))\n",
      " |      ...     output.append(hx)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTMCell\n",
      " |      RNNCellBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_size, hidden_size, bias=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  forward(self, input, hx)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overriden by all subclasses.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNCellBase:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |  \n",
      " |  cuda(self, device_id=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device_id (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all parameters and buffers to double datatype.\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all parameters and buffers to float datatype.\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all parameters and buffers to half datatype.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. The keys of :attr:`state_dict` must\n",
      " |      exactly match the keys returned by this module's :func:`state_dict()`\n",
      " |      function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): A dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          >>>     print(idx, '->', m)\n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time :func:`forward` computes an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='')\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on modules such as Dropout or BatchNorm.\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.LSTMCell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting LSTM from torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features:  \t 100\n",
      "n_samples:   \t 1000\n",
      "hidden_size: \t 200\n"
     ]
    }
   ],
   "source": [
    "n_features = data.shape[0]\n",
    "n_samples  = data.shape[1]\n",
    "hidden_size = 200\n",
    "\n",
    "print(\"n_features:  \\t\", n_features)\n",
    "print(\"n_samples:   \\t\",  n_samples)\n",
    "print(\"hidden_size: \\t\", hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux = torch.nn.LSTM(hidden_size, input_size, num_layers)\n",
    "lstm = torch.nn.LSTM(hidden_size=hidden_size, input_size=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All weights can be found in lstm.all_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lstm.all_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lstm.all_weights[0])):\n",
    "    print(type(lstm.all_weights[0][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 100])\n",
      "torch.Size([800, 200])\n",
      "torch.Size([800])\n",
      "torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "print(lstm.all_weights[0][0].size())\n",
    "print(lstm.all_weights[0][1].size())\n",
    "print(lstm.all_weights[0][2].size())\n",
    "print(lstm.all_weights[0][3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(aux.state_dict()): \t\t 8\n",
      "len(aux.all_weights): \t\t 2\n",
      "length all_weights per layer:    [4, 4]\n"
     ]
    }
   ],
   "source": [
    "aux = torch.nn.LSTM(10, 20, 2)\n",
    "\n",
    "print(\"len(aux.state_dict()): \\t\\t\", len(aux.state_dict()))\n",
    "print(\"len(aux.all_weights): \\t\\t\", len(aux.all_weights))\n",
    "print(\"length all_weights per layer:   \", [len(all_weights) for all_weights in aux.all_weights] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(aux.state_dict()): \t\t 12\n",
      "len(aux.all_weights): \t\t 3\n",
      "length all_weights per layer:    [4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "aux = torch.nn.LSTM(10, 20, 3)\n",
    "\n",
    "print(\"len(aux.state_dict()): \\t\\t\", len(aux.state_dict()))\n",
    "print(\"len(aux.all_weights): \\t\\t\", len(aux.all_weights))\n",
    "print(\"length all_weights per layer:   \", [len(all_weights) for all_weights in aux.all_weights] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "T = 20   \n",
    "L = 1000 \n",
    "N = 100  \n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) +  np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(0.7 * x / 1.0 / T).astype('float64') \n",
    "torch.save(data, open('traindata.pt', 'wb'))\n",
    "\n",
    "data = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1000])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(hidden_size=100, input_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module torch.nn.modules.rnn:\n",
      "\n",
      "forward(input, hx=None) method of torch.nn.modules.rnn.LSTM instance\n",
      "    Defines the computation performed at every call.\n",
      "    \n",
      "    Should be overriden by all subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lstm.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward pass single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = torch.autograd.Variable(torch.rand(500, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[1:2, :]\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = lstm.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 100])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type component 0 lstm.forward: \t <class 'torch.autograd.variable.Variable'>\n",
      "type component 1 lstm.forward: \t <class 'tuple'>\n",
      "\n",
      "\n",
      "size compoment 0 lstm.forward: \t torch.Size([1, 10, 100])\n",
      "len  component 1 lstm.forward: \t 2\n",
      "\n",
      "\n",
      "size compoment 1,0 lstm.forward: \t torch.Size([1, 10, 100])\n",
      "size compoment 1,1 lstm.forward: \t torch.Size([1, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "print(\"type component 0 lstm.forward: \\t\", type(aux[0]))\n",
    "print(\"type component 1 lstm.forward: \\t\", type(aux[1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"size compoment 0 lstm.forward: \\t\", aux[0].size())\n",
    "print(\"len  component 1 lstm.forward: \\t\", len(aux[1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"size compoment 1,0 lstm.forward: \\t\", aux[1][0].size())\n",
    "print(\"size compoment 1,1 lstm.forward: \\t\", aux[1][1].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[0:20, :]\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = lstm.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 10, 100])"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = torch.nn.Linear(100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.4985\n",
       "-0.4067\n",
       "-0.2312\n",
       "-0.4535\n",
       " 0.4957\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(torch.autograd.Variable(torch.rand(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.5502 -0.3546 -0.4592 -0.4234  0.5706\n",
       "-0.6850 -0.1418 -0.0759 -0.2729  0.1040\n",
       "-0.5778 -0.1102  0.1089 -0.4103  0.3017\n",
       "-0.5429  0.0643 -0.1319  0.0228  0.6593\n",
       "-0.5694  0.0661 -0.0495 -0.1381  0.5810\n",
       "-0.3932 -0.0065 -0.0234 -0.4411  0.2993\n",
       "-0.4545  0.2369 -0.0911 -0.3987  0.3119\n",
       "-0.6819 -0.2278 -0.0061 -0.2484  0.2684\n",
       "-0.6012 -0.4910 -0.4075 -0.4303  0.6713\n",
       "-0.9007 -0.0321 -0.1629  0.0637  0.6134\n",
       "[torch.FloatTensor of size 10x5]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(torch.autograd.Variable(torch.rand(10, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-413-1c5b56c5470e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# need a autograd.Variable object to make the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/david/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36maddmm\u001b[0;34m(cls, *args)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAddmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m_blas\u001b[0;34m(cls, args, inplace)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                 \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition"
     ]
    }
   ],
   "source": [
    "# need a autograd.Variable object to make the forward pass\n",
    "mlp.forward(torch.rand(10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1206  0.9776  0.7296  ...   0.8177  0.2970  0.8552\n",
       " 0.7566  0.8856  0.0954  ...   0.0499  0.5040  0.1644\n",
       " 0.2729  0.7349  0.5962  ...   0.2812  0.8640  0.0107\n",
       "          ...             ⋱             ...          \n",
       " 0.3133  0.3792  0.3889  ...   0.0404  0.3056  0.7567\n",
       " 0.2899  0.5734  0.1507  ...   0.3435  0.0414  0.7763\n",
       " 0.4265  0.7858  0.5863  ...   0.6514  0.3039  0.4173\n",
       "[torch.FloatTensor of size 10x100]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
