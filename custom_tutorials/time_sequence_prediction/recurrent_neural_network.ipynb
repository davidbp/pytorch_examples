{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T10:04:15.830144Z",
     "start_time": "2020-05-08T10:04:15.818950Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-068511882983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nNotebook done in pytorch version: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "print(\"\\nNotebook done in pytorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Recurrent unit\n",
    "\n",
    "\n",
    "- $s^{<t-1>}$ activation from previous time $t$\n",
    "- $x^{<t>}$ input to the recurrent unit at time $t$\n",
    "\n",
    "#### How to compute the first hidden state and output \n",
    "\n",
    "We need some initial state for the recurrent unit $s^{<0>}= (0,\\dots,0)$ and an activation function for the hidden state $g_s$ (usually tanh or relu) and activation function for the output  $g_y$ (sigmoid if we have a binary classification problem/ softmax if we have K classes).\n",
    "\n",
    "- The recurrent signal at time $t=1$, $s^{<1>}$  is computed as:\n",
    "$$\n",
    "s^{<1>} = g_s\\left( W_{ss} \\cdot s^{<0>} + W_{xa} \\cdot x^{<1>} + b_s \\right)\n",
    "$$\n",
    "\n",
    "- The output signal at time $t=1$, $\\hat{y \\,}^{<1>}$ is computed as:\n",
    "$$\n",
    "\\hat{y \\,}^{<1>} = g_y\\left( W_{ys} \\cdot s^{<0>} + b_y \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "#### How to compute the hidden state and output at time t\n",
    "\n",
    "- The recurrent signal at time $t$, $s^{<t>}$  is computed as:\n",
    "$$\n",
    "s^{<t>} = g_s\\left( W_{ss} \\cdot s^{<t-1>} + W_{xa} \\cdot x^{<t>} + b_s \\right)\n",
    "$$\n",
    "\n",
    "- The output signal at time $t=1$, $\\hat{y \\,}^{<1>}$ is computed as:\n",
    "$$\n",
    "\\hat{y \\,}^{<t>} = g_y\\left( W_{ys} \\cdot s^{<t>} + b_y \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "rnn = torch.nn.RNN(input_size=6,hidden_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(rnn.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden to hidden weights size: torch.Size([256, 256])\n",
      "hidden to hidden bias  size: torch.Size([256])\n",
      "input to hidden weight matrix size: torch.Size([256, 6])\n",
      "input to hidden bias  size: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden to hidden weights size:\", rnn.weight_hh_l0.size())\n",
    "print(\"hidden to hidden bias  size:\", rnn.bias_hh_l0.size())\n",
    "\n",
    "print(\"input to hidden weight matrix size:\", rnn.weight_ih_l0.size())\n",
    "print(\"input to hidden bias  size:\", rnn.bias_ih_l0.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stacking weight notation\n",
    "\n",
    "Let us assume\n",
    "\n",
    "- $W_{ss}$ is $(100,100)$ matrix.\n",
    "- $W_{sx}$ is $(100,10.000)$ matrix.\n",
    "\n",
    "Then we can stack matrix $W_{ss}$ and $W_{sx}$ horizontally and create $W_s$.\n",
    "The new matrix will have the same number of rows (100) but it will have as many columns as the sum.\n",
    "The notation $W_s = [W_{ss} W_{ax}]$ is usually used to emphasize that matrices have been concatenated size by side and the number columns has increase but the number of rows stays the same. Notice that \n",
    "\n",
    "- $W_{s}$ is $(100,10.100)$ matrix.\n",
    "\n",
    "\n",
    "Let us denote by $[v_1, v_2]$ the vertical concatenation of vectors. If $v_1$ is $(n_1,1)$ and $v_2$ is $(n_2,1)$ then \n",
    "$[v_1, v_2]$ will be $(n_1+n_2,1)$ vector.\n",
    "\n",
    "Using the matrix $W_{s}$  and the previous notation of vector concatenation we can rewrite the forward equations as\n",
    "\n",
    "- The recurrent signal at time $t$, $a^{<t>}$  is computed as:\n",
    "$$\n",
    "s^{<t>} = g_s\\left( W_{s} \\cdot \\left[\\substack{s^{<t-1>} \\\\  x^{<t>}} \\right] + b_s \\right)\n",
    "$$\n",
    "\n",
    "- The output signal at time $t=1$, $y^{<1>}$ is computed as:\n",
    "$$\n",
    "\\hat{y \\,}^{<t>} = g_y\\left( W_{ys} \\cdot   s^{<t>} + b_y \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- We might simply write for the output signal at time $t=1$, \n",
    "$$\n",
    "\\hat{y \\,}^{<t>} = g_y \\left( W_{y} \\cdot   s^{<t>} + b_y \\right)\n",
    "$$\n",
    "\n",
    "Notice that if a state vector had 100 dimensions and an input vector had 10.000 dimensions then $\\left[s^{<t-1>} , x^{<t>}\\right] $ or simply $ \\left[\\substack{s^{<t-1>} \\\\  x^{<t>}} \\right]$ would be a 10.100 dimensional vector. Therefore the multiplication $W_{s} \\cdot \\left[s^{<t-1>} , x^{<t>}\\right]$ or  $W_{s} \\cdot \\left[\\substack{s^{<t-1>} \\\\  x^{<t>}} \\right]$   is a well defined $(100,10.100) \\cdot (10.100,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Gated recurrent unit \n",
    "\n",
    "The GRU has a variable $c$ (memory cell). The memory cell will provide memory to remember past observed things on sequences.\n",
    "\n",
    "The GRU will output $c^{<t>} = s^{<t>}$. We use this notation because we will describe later on the LSTM and it will make sense to use this notation.\n",
    "\n",
    "- At every time step we will consider the value $\\hat{c\\,}^{<t>} $ to be a candidate to replace $c^{<t>}$.\n",
    "The candidate will be computed as\n",
    "\n",
    "$$\\hat{c\\,}^{<t>} = \\tanh \\left( W_c \\cdot  \\left[c^{<t-1>}, x^{<t>} \\right] + b_c \\right)$$ \n",
    "\n",
    "- Which in stacked notation will be \n",
    "\n",
    "$$\\hat{c\\,}^{<t>} = \\tanh \\left( W_c \\cdot  \\left[ \\substack{c^{<t-1>} \\\\ x^{<t>}} \\right] + b_c \\right)$$\n",
    "\n",
    "\n",
    "### Update Gate $\\Gamma_u$\n",
    "\n",
    "The main addition of the GRU with respect to the simple RNN cell is the addition of the update gate $\\Gamma_u$.\n",
    "You can thing about this value as beeing 0 or 1 even though in practise it will be a value between 0 and 1 since it is a sigmoid applied to a vector.\n",
    "\n",
    "\n",
    "The GRU at every time step things about updating  $c^{<t>}$ with $\\hat{c\\,}^{<t>}$. Who will control the update will be the update gate $\\Gamma_u$\n",
    "\n",
    "- The Update gate value $\\Gamma_u$ is computed as\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma \\left( W_u \\cdot \\left[c^{<t-1>}, x^{<t>} \\right] + b_u \\right)\n",
    "$$\n",
    "\n",
    "- The cell state $c^{<t>}$  will be updated as\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u \\odot \\hat{c\\,}^{<t>} .+ ( 1 - \\Gamma_u  ) \\odot c^{<t-1>}\n",
    "$$\n",
    "\n",
    "\n",
    "Notice that $\\dim(\\Gamma_u) = \\dim(\\hat{c\\,}) =  \\dim(c)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated recurrent unit \n",
    "\n",
    "- https://www.youtube.com/watch?v=wSabaLGEegM&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7&index=8\n",
    "\n",
    "- https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "\n",
    "- https://pytorch.org/docs/master/nn.html\n",
    "\n",
    "## Full GRU:\n",
    "\n",
    "- At every time step we will consider the value $\\hat{c\\,}^{<t>} $ to be a candidate to replace $c^{<t>}$.\n",
    "The candidate will be computed as\n",
    "\n",
    "$$\\hat{c\\,}^{<t>} = \\tanh \\left( W_c \\cdot  \\left[ \\Gamma_r  \\odot c^{<t-1>}, x^{<t>} \\right] + b_c \\right)$$ \n",
    "\n",
    "- The Recurrent gate value $\\Gamma_r$ is computed as\n",
    "\n",
    "$$\n",
    "\\Gamma_r = \\sigma \\left( W_r \\cdot \\left[c^{<t-1>}, x^{<t>} \\right] + b_r \\right)\n",
    "$$\n",
    "\n",
    "- The Update gate value $\\Gamma_u$ is computed as\n",
    "\n",
    "$$\n",
    "\\Gamma_u = \\sigma \\left( W_u \\cdot \\left[c^{<t-1>}, x^{<t>} \\right] + b_u \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- The cell state $c^{<t>}$  will be updated as\n",
    "\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u \\odot \\hat{c\\,}^{<t>} .+ ( 1 - \\Gamma_u  ) \\odot c^{<t-1>}\n",
    "$$\n",
    "\n",
    "\n",
    "In summary  $W_c, W_r, W_u$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![GRU_diagram](./GRU_diagram.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "gru = torch.nn.GRU(input_size=6, hidden_size=256)\n",
    "sample = torch.autograd.Variable(torch.Tensor(np.random.rand(6).reshape(1,1,6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1915, 0.6221, 0.4377, 0.7854, 0.7800, 0.2726]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gru.forward(sample)), len(gru.forward(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = gru.forward(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size(), b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(gru.state_dict().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden to hidden weights size: torch.Size([768, 256])\n",
      "hidden to hidden bias  size: torch.Size([768])\n",
      "input to hidden weight matrix size: torch.Size([768, 6])\n",
      "input to hidden bias  size: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(\"hidden to hidden weights size:\", gru.weight_hh_l0.size())\n",
    "print(\"hidden to hidden bias  size:\", gru.bias_hh_l0.size())\n",
    "\n",
    "print(\"input to hidden weight matrix size:\", gru.weight_ih_l0.size())\n",
    "print(\"input to hidden bias  size:\", gru.bias_ih_l0.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
